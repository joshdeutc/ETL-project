{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd043a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mario\\OneDrive\\Documents\\UTC\\RT05\\NF26\\nf26_project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "725309fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in c:\\users\\mario\\anaconda3\\envs\\ids\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\mario\\anaconda3\\envs\\ids\\lib\\site-packages (from geopy) (2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a821f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from etl_warehouse import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccebe900",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NF26_Project\") \\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"Europe/Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec4d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Définition du fuseau horaire sur paris ##\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"Europe/Paris\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec663d",
   "metadata": {},
   "source": [
    "TABLE PERSONNEL\n",
    "pas besoin de la créer de manière journalière car on pouvait l'extraire directement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a68047c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------------+----------+-----------+-----------+-----------+-----------------+-------------+--------+----------+---------+---------+------+-------+------------------+---------------------+-------------------+\n",
      "|        ID_PERSONNEL|NOM_PERSONNEL|PRENOM_PERSONNEL|  DT_NAISS|VILLE_NAISS| PAYS_NAISS|   NUM_SECU|IND_PAYS_NUM_TELP|NUM_TELEPHONE|NUM_VOIE|  DSC_VOIE|CMPL_VOIE|CD_POSTAL| VILLE|   PAYS|FONCTION_PERSONNEL|TS_CREATION_PERSONNEL|  TS_MAJ_PPERSONNEL|\n",
      "+--------------------+-------------+----------------+----------+-----------+-----------+-----------+-----------------+-------------+--------+----------+---------+---------+------+-------+------------------+---------------------+-------------------+\n",
      "|KeyPers_Berlin_12...|        Name0|       FistName0|1993-11-04|      Osaka|      Japan|NS000000000|             NULL|+336##0263188|      57|NomVoie940|     NULL|    #8830|BERLIN|Germany|    Dateningenieur|  2010-09-09 11:22:17|2010-09-09 11:22:17|\n",
      "|KeyPers_Berlin_12...|        Name1|       FistName1|1932-11-22| Wellington|New Zealand|NS000000001|             NULL|+336##0401873|      64|NomVoie711|     NULL|    #9785|BERLIN|Germany|     Führungskraft|  2017-07-24 16:51:18|2017-07-24 16:51:18|\n",
      "|KeyPers_Berlin_12...|        Name2|       FistName2|1990-08-12|     Sidney|  Australia|NS000000002|             NULL|+336##0524126|      94|NomVoie322|     NULL|    #4816|BERLIN|Germany|    Dateningenieur|  1997-08-20 04:01:46|1997-08-20 04:01:46|\n",
      "|KeyPers_Berlin_12...|        Name3|       FistName3|1965-05-26|      Rabat|      Maroc|NS000000003|             NULL|+336##0418484|      78|NomVoie593|     NULL|    #3546|BERLIN|Germany|            Ökonom|  1998-11-17 19:25:01|1998-11-17 19:25:01|\n",
      "|KeyPers_Berlin_12...|        Name4|       FistName4|1959-01-18|   Shanghai|      China|NS000000004|             NULL|+336##0986317|      65|NomVoie404|     NULL|    #6788|BERLIN|Germany|    Dateningenieur|  2010-11-10 04:24:37|2010-11-10 04:24:37|\n",
      "|KeyPers_Berlin_12...|        Name5|       FistName5|1985-10-08|      Dubaï|    Emirats|NS000000005|             NULL|+336##0270257|      36|NomVoie325|     NULL|    #3254|BERLIN|Germany|     Führungskraft|  2009-07-21 13:37:06|2009-07-21 13:37:06|\n",
      "|KeyPers_Berlin_12...|        Name6|       FistName6|1953-04-13|      Lille|     France|       NULL|             NULL|+336##0771894|      96|NomVoie126|     NULL|    #5405|BERLIN|Germany|            Ökonom|  2011-11-26 01:26:55|2011-11-26 01:26:55|\n",
      "+--------------------+-------------+----------------+----------+-----------+-----------+-----------+-----------------+-------------+--------+----------+---------+---------+------+-------+------------------+---------------------+-------------------+\n",
      "only showing top 7 rows\n",
      "\n",
      "Table dimension PERSONNEL créée avec 20572 employés\n"
     ]
    }
   ],
   "source": [
    "#BERLIN\n",
    "psdf_personnel_BERLIN = ps.read_csv('BDD_BGES/BDD_BGES/BDD_BGES_BERLIN/PERSONNEL_BERLIN.txt', index_col='ID_PERSONNEL', sep=';', encoding='utf-8')\n",
    "sdf_personnel_BERLIN = psdf_personnel_BERLIN.to_spark(index_col='ID_PERSONNEL')\n",
    "\n",
    "#LONDON\n",
    "psdf_personnel_LONDON = ps.read_csv('BDD_BGES/BDD_BGES/BDD_BGES_LONDON/PERSONNEL_LONDON.txt', index_col='ID_PERSONNEL', sep=';', encoding='utf-8')\n",
    "sdf_personnel_LONDON = psdf_personnel_LONDON.to_spark(index_col='ID_PERSONNEL')\n",
    "\n",
    "#LOSANGELES\n",
    "psdf_personnel_LOSANGELES = ps.read_csv('BDD_BGES/BDD_BGES/BDD_BGES_LOSANGELES/PERSONNEL_LOSANGELES.txt', index_col='ID_PERSONNEL', sep=';', encoding='utf-8')\n",
    "sdf_personnel_LOSANGELES = psdf_personnel_LOSANGELES.to_spark(index_col='ID_PERSONNEL')\n",
    "\n",
    "#NEWYORK\n",
    "psdf_personnel_NEWYORK = ps.read_csv('BDD_BGES/BDD_BGES/BDD_BGES_NEWYORK/PERSONNEL_NEWYORK.txt', index_col='ID_PERSONNEL', sep=';', encoding='utf-8')\n",
    "sdf_personnel_NEWYORK = psdf_personnel_NEWYORK.to_spark(index_col='ID_PERSONNEL')\n",
    "\n",
    "#PARIS\n",
    "psdf_personnel_PARIS = ps.read_csv('BDD_BGES/BDD_BGES/BDD_BGES_PARIS/PERSONNEL_PARIS.txt', index_col='ID_PERSONNEL', sep=';', encoding='utf-8')\n",
    "sdf_personnel_PARIS = psdf_personnel_PARIS.to_spark(index_col='ID_PERSONNEL')\n",
    "\n",
    "#SHANGHAI\n",
    "psdf_personnel_SHANGHAI = ps.read_csv('BDD_BGES/BDD_BGES/BDD_BGES_SHANGHAI/PERSONNEL_SHANGHAI.txt', index_col='ID_PERSONNEL', sep=';', encoding='utf-8')\n",
    "sdf_personnel_SHANGHAI = psdf_personnel_SHANGHAI.to_spark(index_col='ID_PERSONNEL')\n",
    " \n",
    " \n",
    " \n",
    "# Code corrigé pour la dimension personnel\n",
    "import os\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "# Créer le répertoire DIM_PERSONNEL s'il n'existe pas\n",
    "os.makedirs(\"DATA_WAREHOUSE/DIM_PERSONNEL\", exist_ok=True)\n",
    "\n",
    "# Union de tous les DataFrames du personnel\n",
    "sdf_personnel_all = sdf_personnel_BERLIN.union(sdf_personnel_LONDON) \\\n",
    "    .union(sdf_personnel_LOSANGELES).union(sdf_personnel_NEWYORK) \\\n",
    "    .union(sdf_personnel_PARIS).union(sdf_personnel_SHANGHAI)\n",
    "\n",
    "# Ajouter la colonne VILLE pour indiquer la provenance\n",
    "sdf_personnel_berlin_with_city = sdf_personnel_BERLIN.withColumn(\"VILLE\", lit(\"BERLIN\"))\n",
    "sdf_personnel_london_with_city = sdf_personnel_LONDON.withColumn(\"VILLE\", lit(\"LONDON\"))\n",
    "sdf_personnel_la_with_city = sdf_personnel_LOSANGELES.withColumn(\"VILLE\", lit(\"LOSANGELES\"))\n",
    "sdf_personnel_ny_with_city = sdf_personnel_NEWYORK.withColumn(\"VILLE\", lit(\"NEWYORK\"))\n",
    "sdf_personnel_paris_with_city = sdf_personnel_PARIS.withColumn(\"VILLE\", lit(\"PARIS\"))\n",
    "sdf_personnel_shanghai_with_city = sdf_personnel_SHANGHAI.withColumn(\"VILLE\", lit(\"SHANGHAI\"))\n",
    "\n",
    "# Union de tous les DataFrames avec la colonne VILLE\n",
    "sdf_personnel_all_with_city = sdf_personnel_berlin_with_city.union(sdf_personnel_london_with_city) \\\n",
    "    .union(sdf_personnel_la_with_city).union(sdf_personnel_ny_with_city) \\\n",
    "    .union(sdf_personnel_paris_with_city).union(sdf_personnel_shanghai_with_city)\n",
    "\n",
    "# Afficher un échantillon\n",
    "sdf_personnel_all_with_city.show(7)\n",
    "\n",
    "# Spécifier le chemin complet du fichier CSV (pas seulement le dossier)\n",
    "dim_personnel_path = \"DATA_WAREHOUSE/DIM_PERSONNEL/dimension_personnel.csv\"\n",
    "\n",
    "# Utiliser pandas pour sauvegarder en un seul fichier\n",
    "personnel_pandas_df = sdf_personnel_all_with_city.toPandas()\n",
    "personnel_pandas_df.to_csv(dim_personnel_path, index=False)\n",
    "\n",
    "print(f\"Table dimension PERSONNEL créée avec {sdf_personnel_all_with_city.count()} employés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6fa5f",
   "metadata": {},
   "source": [
    "EXECUTION ETL boucle avec tous les jours de 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c73019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_full_year()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da28aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage de l'ETL pour la date: 20240429\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_BERLIN\\BDD_BGES_BERLIN_MISSION\\MISSION_20240429.txt - 27 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_PARIS\\BDD_BGES_PARIS_MISSION\\MISSION_20240429.txt - 33 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LONDON\\BDD_BGES_LONDON_MISSION\\MISSION_20240429.txt - 16 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LOSANGELES\\BDD_BGES_LOSANGELES_MISSION\\MISSION_20240429.txt - 12 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_NEWYORK\\BDD_BGES_NEWYORK_MISSION\\MISSION_20240429.txt - 29 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_SHANGHAI\\BDD_BGES_SHANGHAI_MISSION\\MISSION_20240429.txt - 15 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_SHANGHAI\\BDD_BGES_SHANGHAI_MISSION\\MISSION_20240430.txt - 5 lignes\n",
      "Table dimension DATE créée avec 1 dates\n",
      "premiere mission\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mario\\OneDrive\\Documents\\UTC\\RT05\\NF26\\nf26_project\\distance.py:54: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cache_df = pd.concat([cache_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Buenos Aires\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Osaka\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Bogota\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Rio de Janeiro\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Buenos Aires\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Bogota\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Tokyo\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "premiere mission\n",
      "error numero : 0 avec la query Los Angeles\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Sidney\n",
      "premiere mission\n",
      "error numero : 0 avec la query Dubaï\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Osaka\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Los Angeles\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Los Angeles\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Los Angeles\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Buenos Aires\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Oslo\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Dubaï\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "error numero : 1 avec la query Shanghai\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "error numero : 1 avec la query Shanghai\n",
      "error numero : 2 avec la query Shanghai\n",
      "error numero : 3 avec la query Shanghai\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "error numero : 1 avec la query Shanghai\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "premiere mission\n",
      "premiere mission\n",
      "error numero : 0 avec la query Shanghai\n",
      "Table dimension MISSION créée avec 137 missions\n",
      "Table de faits MISSION créée avec 137 lignes pour la date 20240429\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_BERLIN\\BDD_BGES_BERLIN_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240429.txt - 1 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_PARIS\\BDD_BGES_PARIS_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240429.txt - 8 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LONDON\\BDD_BGES_LONDON_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240429.txt - 7 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LOSANGELES\\BDD_BGES_LOSANGELES_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240429.txt - 3 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_NEWYORK\\BDD_BGES_NEWYORK_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240429.txt - 1 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_SHANGHAI\\BDD_BGES_SHANGHAI_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240429.txt - 1 lignes\n",
      "Table dimension DATE mise à jour avec 1 nouvelles dates\n",
      "Table de faits MATERIEL créée avec 21 lignes pour la date 20240429\n",
      "ETL terminé pour la date: 20240429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"date2 = '20240907'\\nexecuter_etl_pour_date(date2)\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date1 = '20240429'\n",
    "executer_etl_pour_date(date1)\n",
    "\"\"\"date2 = '20240907'\n",
    "executer_etl_pour_date(date2)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd781255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13afbee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/c:/Users/mario/OneDrive/Documents/UTC/RT05/NF26/nf26_project/DATA_WAREHOUSE/DIM_MATERIEL/dimension_materiel.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n",
      "\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Charger les tables de dimension et de faits\u001b[39;00m\n",
      "\u001b[0;32m      6\u001b[0m dim_personnel \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA_WAREHOUSE/DIM_PERSONNEL/dimension_personnel.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;32m----> 7\u001b[0m dim_materiel \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDATA_WAREHOUSE/DIM_MATERIEL/dimension_materiel.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n",
      "\u001b[0;32m      8\u001b[0m dim_date \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA_WAREHOUSE/DIM_DATE/dimension_date.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;32m      9\u001b[0m faits_materiel \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA_WAREHOUSE/FAITS_MATERIEL/faits_materiel.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mario\\anaconda3\\envs\\ids\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n",
      "\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n",
      "\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mario\\anaconda3\\envs\\ids\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n",
      "\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
      "\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
      "\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
      "\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
      "\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mario\\anaconda3\\envs\\ids\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n",
      "\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
      "\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
      "\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
      "\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/c:/Users/mario/OneDrive/Documents/UTC/RT05/NF26/nf26_project/DATA_WAREHOUSE/DIM_MATERIEL/dimension_materiel.csv."
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Charger les tables de dimension et de faits\n",
    "dim_personnel = spark.read.csv(\"DATA_WAREHOUSE/DIM_PERSONNEL/dimension_personnel.csv\", header=True, inferSchema=True)\n",
    "dim_materiel = spark.read.csv(\"DATA_WAREHOUSE/DIM_MATERIEL/dimension_materiel.csv\", header=True, inferSchema=True) \n",
    "dim_date = spark.read.csv(\"DATA_WAREHOUSE/DIM_DATE/dimension_date.csv\", header=True, inferSchema=True)\n",
    "faits_materiel = spark.read.csv(\"DATA_WAREHOUSE/FAITS_MATERIEL/faits_materiel.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Examiner le schéma des tables pour comprendre les colonnes disponibles\n",
    "print(\"Schema de la table PERSONNEL:\")\n",
    "dim_personnel.printSchema()\n",
    "\n",
    "# Créer les vues pour les requêtes SQL\n",
    "dim_personnel.createOrReplaceTempView(\"personnel\")\n",
    "dim_materiel.createOrReplaceTempView(\"materiel\")\n",
    "dim_date.createOrReplaceTempView(\"dates\")\n",
    "faits_materiel.createOrReplaceTempView(\"faits_materiel\")\n",
    "\n",
    "# Question 1: Combien d'ingénieurs informaticiens travaillent sur le site de Paris?\n",
    "q1 = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as nb_ingenieurs_info_paris\n",
    "    FROM personnel\n",
    "    WHERE FONCTION_PERSONNEL = 'Ingénieur Informaticien' \n",
    "    AND VILLE = 'PARIS'\n",
    "\"\"\")\n",
    "q1.show()\n",
    "\n",
    "# Question 2: Combien d'ingénieurs Data travaillent sur les sites de London?\n",
    "q2 = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as nb_ingenieurs_data_london\n",
    "    FROM personnel\n",
    "    WHERE FONCTION_PERSONNEL = 'Data Engineer' \n",
    "    AND VILLE = 'LONDON'\n",
    "\"\"\")\n",
    "q2.show()\n",
    "\n",
    "# Question 3: Combien de cadres travaillent dans l'organisation (tous sites compris)?\n",
    "q3 = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as nb_cadres_total\n",
    "    FROM personnel\n",
    "    WHERE FONCTION_PERSONNEL LIKE '%Cadre%'  \n",
    "      OR FONCTION_PERSONNEL LIKE '%Führungskraft%' \n",
    "      OR FONCTION_PERSONNEL LIKE '%Business Executive%'\n",
    "\"\"\")\n",
    "q3.show()\n",
    "\n",
    "# Question 4: Combien de PC portables ont été achetés par l'organisation entre mai et octobre 2024?\n",
    "# D'abord, vérifions le schéma des tables pour cette requête\n",
    "print(\"Schema de la table MATERIEL:\")\n",
    "dim_materiel.printSchema()\n",
    "print(\"Schema de la table DATES:\")\n",
    "dim_date.printSchema()\n",
    "print(\"Schema de la table FAITS_MATERIEL:\")\n",
    "faits_materiel.printSchema()\n",
    "\n",
    "q4 = spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT f.ID_MATERIELINFO) as nb_pc_portables\n",
    "    FROM faits_materiel f\n",
    "    JOIN materiel m ON f.ID_MATERIELINFO = m.ID_MATERIELINFO\n",
    "    JOIN dates d ON f.KeyDate = d.KeyDate\n",
    "    WHERE m.TYPE LIKE '%PC%porta%'\n",
    "    AND d.MOIS BETWEEN 5 AND 10\n",
    "    AND d.ANNEE = 2024\n",
    "\"\"\")\n",
    "q4.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
