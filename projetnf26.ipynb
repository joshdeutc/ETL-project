{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd043a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utcpret\\Documents\\NF26\\nf26_project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "725309fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in c:\\users\\utcpret\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\utcpret\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from geopy) (2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\utcpret\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a821f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utcpret\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\pandas\\utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "C:\\Users\\utcpret\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\pandas\\utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from etl_warehouse import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccebe900",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec4d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Définition du fuseau horaire sur paris ##\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"Europe/Paris\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec663d",
   "metadata": {},
   "source": [
    "TABLE PERSONNEL\n",
    "pas besoin de la créer de manière journalière car on pouvait l'extraire directement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a68047c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------------+----------+-----------+-----------+-----------+-----------------+-------------+--------+----------+---------+---------+------+-------+------------------+---------------------+-------------------+\n",
      "|        ID_PERSONNEL|NOM_PERSONNEL|PRENOM_PERSONNEL|  DT_NAISS|VILLE_NAISS| PAYS_NAISS|   NUM_SECU|IND_PAYS_NUM_TELP|NUM_TELEPHONE|NUM_VOIE|  DSC_VOIE|CMPL_VOIE|CD_POSTAL| VILLE|   PAYS|FONCTION_PERSONNEL|TS_CREATION_PERSONNEL|  TS_MAJ_PPERSONNEL|\n",
      "+--------------------+-------------+----------------+----------+-----------+-----------+-----------+-----------------+-------------+--------+----------+---------+---------+------+-------+------------------+---------------------+-------------------+\n",
      "|KeyPers_Berlin_12...|        Name0|       FistName0|1993-11-04|      Osaka|      Japan|NS000000000|             NULL|+336##0263188|      57|NomVoie940|     NULL|    #8830|BERLIN|Germany|    Dateningenieur|  2010-09-09 11:22:17|2010-09-09 11:22:17|\n",
      "|KeyPers_Berlin_12...|        Name1|       FistName1|1932-11-22| Wellington|New Zealand|NS000000001|             NULL|+336##0401873|      64|NomVoie711|     NULL|    #9785|BERLIN|Germany|     Führungskraft|  2017-07-24 16:51:18|2017-07-24 16:51:18|\n",
      "|KeyPers_Berlin_12...|        Name2|       FistName2|1990-08-12|     Sidney|  Australia|NS000000002|             NULL|+336##0524126|      94|NomVoie322|     NULL|    #4816|BERLIN|Germany|    Dateningenieur|  1997-08-20 04:01:46|1997-08-20 04:01:46|\n",
      "|KeyPers_Berlin_12...|        Name3|       FistName3|1965-05-26|      Rabat|      Maroc|NS000000003|             NULL|+336##0418484|      78|NomVoie593|     NULL|    #3546|BERLIN|Germany|            Ökonom|  1998-11-17 19:25:01|1998-11-17 19:25:01|\n",
      "|KeyPers_Berlin_12...|        Name4|       FistName4|1959-01-18|   Shanghai|      China|NS000000004|             NULL|+336##0986317|      65|NomVoie404|     NULL|    #6788|BERLIN|Germany|    Dateningenieur|  2010-11-10 04:24:37|2010-11-10 04:24:37|\n",
      "|KeyPers_Berlin_12...|        Name5|       FistName5|1985-10-08|      Dubaï|    Emirats|NS000000005|             NULL|+336##0270257|      36|NomVoie325|     NULL|    #3254|BERLIN|Germany|     Führungskraft|  2009-07-21 13:37:06|2009-07-21 13:37:06|\n",
      "|KeyPers_Berlin_12...|        Name6|       FistName6|1953-04-13|      Lille|     France|       NULL|             NULL|+336##0771894|      96|NomVoie126|     NULL|    #5405|BERLIN|Germany|            Ökonom|  2011-11-26 01:26:55|2011-11-26 01:26:55|\n",
      "+--------------------+-------------+----------------+----------+-----------+-----------+-----------+-----------------+-------------+--------+----------+---------+---------+------+-------+------------------+---------------------+-------------------+\n",
      "only showing top 7 rows\n",
      "\n",
      "Table dimension PERSONNEL créée avec 20572 employés\n"
     ]
    }
   ],
   "source": [
    "#BERLIN\n",
    "psdf_personnel_BERLIN = ps.read_csv('BDD_BGES/BDD_BGES/BDD_BGES_BERLIN/PERSONNEL_BERLIN.txt', index_col='ID_PERSONNEL', sep=';', encoding='utf-8')\n",
    "sdf_personnel_BERLIN = psdf_personnel_BERLIN.to_spark(index_col='ID_PERSONNEL')\n",
    "\n",
    "#LONDON\n",
    "psdf_personnel_LONDON = ps.read_csv('BDD_BGES/BDD_BGES/BDD_BGES_LONDON/PERSONNEL_LONDON.txt', index_col='ID_PERSONNEL', sep=';', encoding='utf-8')\n",
    "sdf_personnel_LONDON = psdf_personnel_LONDON.to_spark(index_col='ID_PERSONNEL')\n",
    "\n",
    "#LOSANGELES\n",
    "psdf_personnel_LOSANGELES = ps.read_csv('BDD_BGES/BDD_BGES/BDD_BGES_LOSANGELES/PERSONNEL_LOSANGELES.txt', index_col='ID_PERSONNEL', sep=';', encoding='utf-8')\n",
    "sdf_personnel_LOSANGELES = psdf_personnel_LOSANGELES.to_spark(index_col='ID_PERSONNEL')\n",
    "\n",
    "#NEWYORK\n",
    "psdf_personnel_NEWYORK = ps.read_csv('BDD_BGES/BDD_BGES/BDD_BGES_NEWYORK/PERSONNEL_NEWYORK.txt', index_col='ID_PERSONNEL', sep=';', encoding='utf-8')\n",
    "sdf_personnel_NEWYORK = psdf_personnel_NEWYORK.to_spark(index_col='ID_PERSONNEL')\n",
    "\n",
    "#PARIS\n",
    "psdf_personnel_PARIS = ps.read_csv('BDD_BGES/BDD_BGES/BDD_BGES_PARIS/PERSONNEL_PARIS.txt', index_col='ID_PERSONNEL', sep=';', encoding='utf-8')\n",
    "sdf_personnel_PARIS = psdf_personnel_PARIS.to_spark(index_col='ID_PERSONNEL')\n",
    "\n",
    "#SHANGHAI\n",
    "psdf_personnel_SHANGHAI = ps.read_csv('BDD_BGES/BDD_BGES/BDD_BGES_SHANGHAI/PERSONNEL_SHANGHAI.txt', index_col='ID_PERSONNEL', sep=';', encoding='utf-8')\n",
    "sdf_personnel_SHANGHAI = psdf_personnel_SHANGHAI.to_spark(index_col='ID_PERSONNEL')\n",
    " \n",
    " \n",
    " \n",
    "# Code corrigé pour la dimension personnel\n",
    "import os\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "# Créer le répertoire DIM_PERSONNEL s'il n'existe pas\n",
    "os.makedirs(\"DATA_WAREHOUSE/DIM_PERSONNEL\", exist_ok=True)\n",
    "\n",
    "# Union de tous les DataFrames du personnel\n",
    "sdf_personnel_all = sdf_personnel_BERLIN.union(sdf_personnel_LONDON) \\\n",
    "    .union(sdf_personnel_LOSANGELES).union(sdf_personnel_NEWYORK) \\\n",
    "    .union(sdf_personnel_PARIS).union(sdf_personnel_SHANGHAI)\n",
    "\n",
    "# Ajouter la colonne VILLE pour indiquer la provenance\n",
    "sdf_personnel_berlin_with_city = sdf_personnel_BERLIN.withColumn(\"VILLE\", lit(\"BERLIN\"))\n",
    "sdf_personnel_london_with_city = sdf_personnel_LONDON.withColumn(\"VILLE\", lit(\"LONDON\"))\n",
    "sdf_personnel_la_with_city = sdf_personnel_LOSANGELES.withColumn(\"VILLE\", lit(\"LOSANGELES\"))\n",
    "sdf_personnel_ny_with_city = sdf_personnel_NEWYORK.withColumn(\"VILLE\", lit(\"NEWYORK\"))\n",
    "sdf_personnel_paris_with_city = sdf_personnel_PARIS.withColumn(\"VILLE\", lit(\"PARIS\"))\n",
    "sdf_personnel_shanghai_with_city = sdf_personnel_SHANGHAI.withColumn(\"VILLE\", lit(\"SHANGHAI\"))\n",
    "\n",
    "# Union de tous les DataFrames avec la colonne VILLE\n",
    "sdf_personnel_all_with_city = sdf_personnel_berlin_with_city.union(sdf_personnel_london_with_city) \\\n",
    "    .union(sdf_personnel_la_with_city).union(sdf_personnel_ny_with_city) \\\n",
    "    .union(sdf_personnel_paris_with_city).union(sdf_personnel_shanghai_with_city)\n",
    "\n",
    "# Afficher un échantillon\n",
    "sdf_personnel_all_with_city.show(7)\n",
    "\n",
    "# Spécifier le chemin complet du fichier CSV (pas seulement le dossier)\n",
    "dim_personnel_path = \"DATA_WAREHOUSE/DIM_PERSONNEL/dimension_personnel.csv\"\n",
    "\n",
    "# Utiliser pandas pour sauvegarder en un seul fichier\n",
    "personnel_pandas_df = sdf_personnel_all_with_city.toPandas()\n",
    "personnel_pandas_df.to_csv(dim_personnel_path, index=False)\n",
    "\n",
    "print(f\"Table dimension PERSONNEL créée avec {sdf_personnel_all_with_city.count()} employés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6fa5f",
   "metadata": {},
   "source": [
    "EXECUTION ETL boucle avec tous les jours de 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c73019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_full_year()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c425d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.pandas as ps\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.types import *\n",
    "\n",
    "# # Charger les tables de dimension et de faits\n",
    "# dim_personnel = spark.read.csv(\"DATA_WAREHOUSE/DIM_PERSONNEL/dimension_personnel.csv\", header=True, inferSchema=True)\n",
    "# dim_materiel = spark.read.csv(\"DATA_WAREHOUSE/DIM_MATERIEL/dimension_materiel.csv\", header=True, inferSchema=True) \n",
    "# dim_date = spark.read.csv(\"DATA_WAREHOUSE/DIM_DATE/dimension_date.csv\", header=True, inferSchema=True)\n",
    "# faits_materiel = spark.read.csv(\"DATA_WAREHOUSE/FAITS_MATERIEL/faits_materiel.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# # Examiner le schéma des tables pour comprendre les colonnes disponibles\n",
    "# print(\"Schema de la table PERSONNEL:\")\n",
    "# dim_personnel.printSchema()\n",
    "\n",
    "# # Créer les vues pour les requêtes SQL\n",
    "# dim_personnel.createOrReplaceTempView(\"personnel\")\n",
    "# dim_materiel.createOrReplaceTempView(\"materiel\")\n",
    "# dim_date.createOrReplaceTempView(\"dates\")\n",
    "# faits_materiel.createOrReplaceTempView(\"faits_materiel\")\n",
    "\n",
    "# # Question 1: Combien d'ingénieurs informaticiens travaillent sur le site de Paris?\n",
    "# q1 = spark.sql(\"\"\"\n",
    "#     SELECT COUNT(*) as nb_ingenieurs_info_paris\n",
    "#     FROM personnel\n",
    "#     WHERE FONCTION_PERSONNEL = 'Ingénieur Informaticien' \n",
    "#     AND VILLE = 'PARIS'\n",
    "# \"\"\")\n",
    "# q1.show()\n",
    "\n",
    "# # Question 2: Combien d'ingénieurs Data travaillent sur les sites de London?\n",
    "# q2 = spark.sql(\"\"\"\n",
    "#     SELECT COUNT(*) as nb_ingenieurs_data_london\n",
    "#     FROM personnel\n",
    "#     WHERE FONCTION_PERSONNEL = 'Data Engineer' \n",
    "#     AND VILLE = 'LONDON'\n",
    "# \"\"\")\n",
    "# q2.show()\n",
    "\n",
    "# # Question 3: Combien de cadres travaillent dans l'organisation (tous sites compris)?\n",
    "# q3 = spark.sql(\"\"\"\n",
    "#     SELECT COUNT(*) as nb_cadres_total\n",
    "#     FROM personnel\n",
    "#     WHERE FONCTION_PERSONNEL LIKE '%Cadre%'  \n",
    "#       OR FONCTION_PERSONNEL LIKE '%Führungskraft%' \n",
    "#       OR FONCTION_PERSONNEL LIKE '%Business Executive%'\n",
    "# \"\"\")\n",
    "# q3.show()\n",
    "\n",
    "# # Question 4: Combien de PC portables ont été achetés par l'organisation entre mai et octobre 2024?\n",
    "# # D'abord, vérifions le schéma des tables pour cette requête\n",
    "# print(\"Schema de la table MATERIEL:\")\n",
    "# dim_materiel.printSchema()\n",
    "# print(\"Schema de la table DATES:\")\n",
    "# dim_date.printSchema()\n",
    "# print(\"Schema de la table FAITS_MATERIEL:\")\n",
    "# faits_materiel.printSchema()\n",
    "\n",
    "# q4 = spark.sql(\"\"\"\n",
    "#     SELECT COUNT(DISTINCT f.ID_MATERIELINFO) as nb_pc_portables\n",
    "#     FROM faits_materiel f\n",
    "#     JOIN materiel m ON f.ID_MATERIELINFO = m.ID_MATERIELINFO\n",
    "#     JOIN dates d ON f.KeyDate = d.KeyDate\n",
    "#     WHERE m.TYPE LIKE '%PC%porta%'\n",
    "#     AND d.MOIS BETWEEN 5 AND 10\n",
    "#     AND d.ANNEE = 2024\n",
    "# \"\"\")\n",
    "# q4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da28aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage de l'ETL pour la date: 20240429\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_BERLIN\\BDD_BGES_BERLIN_MISSION\\MISSION_20240429.txt - 27 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_PARIS\\BDD_BGES_PARIS_MISSION\\MISSION_20240429.txt - 33 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LONDON\\BDD_BGES_LONDON_MISSION\\MISSION_20240429.txt - 16 lignes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LOSANGELES\\BDD_BGES_LOSANGELES_MISSION\\MISSION_20240429.txt - 12 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_NEWYORK\\BDD_BGES_NEWYORK_MISSION\\MISSION_20240429.txt - 29 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_SHANGHAI\\BDD_BGES_SHANGHAI_MISSION\\MISSION_20240429.txt - 15 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_SHANGHAI\\BDD_BGES_SHANGHAI_MISSION\\MISSION_20240430.txt - 5 lignes\n",
      "Table dimension DATE créée avec 1 dates\n",
      "Geocoder error (attempt 1) for 'Shanghai': HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Shanghai&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "Erreur lors de la création/mise à jour de la table MISSION: can't multiply sequence by non-int of type 'float'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\utcpret\\Documents\\NF26\\nf26_project\\etl_warehouse.py\", line 230, in creer_table_fait_mission\n",
      "    creer_dimension_mission(df_missions)\n",
      "  File \"c:\\Users\\utcpret\\Documents\\NF26\\nf26_project\\etl_warehouse.py\", line 182, in creer_dimension_mission\n",
      "    dim_mission_nouvelles = mission_bilan_carbone(dim_mission_nouvelles)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\utcpret\\Documents\\NF26\\nf26_project\\mission_ges.py\", line 88, in mission_bilan_carbone\n",
      "    GES = GES * 0.001\n",
      "          ~~~~^~~~~~~\n",
      "TypeError: can't multiply sequence by non-int of type 'float'\n",
      "\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_BERLIN\\BDD_BGES_BERLIN_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240429.txt - 1 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_PARIS\\BDD_BGES_PARIS_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240429.txt - 8 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LONDON\\BDD_BGES_LONDON_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240429.txt - 7 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LOSANGELES\\BDD_BGES_LOSANGELES_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240429.txt - 3 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_NEWYORK\\BDD_BGES_NEWYORK_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240429.txt - 1 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_SHANGHAI\\BDD_BGES_SHANGHAI_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240429.txt - 1 lignes\n",
      "Table dimension DATE mise à jour avec 1 nouvelles dates\n",
      "Erreur lors de la création/mise à jour de la table MATERIEL: 'DataFrame' object has no attribute 'to_csv'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\utcpret\\Documents\\NF26\\nf26_project\\etl_warehouse.py\", line 291, in creer_table_fait_materiel\n",
      "    creer_dimension_materiel(df_materiel)\n",
      "  File \"c:\\Users\\utcpret\\Documents\\NF26\\nf26_project\\etl_warehouse.py\", line 213, in creer_dimension_materiel\n",
      "    dim_materiel_nouvelles.to_csv(dimension_materiel_path, index=False)\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\utcpret\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\dataframe.py\", line 3129, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'DataFrame' object has no attribute 'to_csv'\n",
      "\n",
      "ETL terminé pour la date: 20240429\n",
      "Démarrage de l'ETL pour la date: 20240907\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LONDON\\BDD_BGES_LONDON_MISSION\\MISSION_20240906.txt - 1 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LOSANGELES\\BDD_BGES_LOSANGELES_MISSION\\MISSION_20240906.txt - 13 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_NEWYORK\\BDD_BGES_NEWYORK_MISSION\\MISSION_20240906.txt - 3 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_BERLIN\\BDD_BGES_BERLIN_MISSION\\MISSION_20240907.txt - 16 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_PARIS\\BDD_BGES_PARIS_MISSION\\MISSION_20240907.txt - 20 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LONDON\\BDD_BGES_LONDON_MISSION\\MISSION_20240907.txt - 13 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LOSANGELES\\BDD_BGES_LOSANGELES_MISSION\\MISSION_20240907.txt - 2 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_NEWYORK\\BDD_BGES_NEWYORK_MISSION\\MISSION_20240907.txt - 14 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_SHANGHAI\\BDD_BGES_SHANGHAI_MISSION\\MISSION_20240907.txt - 13 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_SHANGHAI\\BDD_BGES_SHANGHAI_MISSION\\MISSION_20240908.txt - 4 lignes\n",
      "Table dimension DATE mise à jour avec 1 nouvelles dates\n",
      "Geocoder error (attempt 1) for 'Los Angeles': HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Los+Angeles&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "Geocoder error (attempt 1) for 'Los Angeles': HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Los+Angeles&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "Geocoder error (attempt 1) for 'Los Angeles': HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Los+Angeles&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "Geocoder error (attempt 1) for 'Los Angeles': HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Los+Angeles&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "Geocoder error (attempt 1) for 'Los Angeles': HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Los+Angeles&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "Geocoder error (attempt 1) for 'Los Angeles': HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Los+Angeles&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "Geocoder error (attempt 1) for 'Los Angeles': HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Los+Angeles&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "Geocoder error (attempt 1) for 'Los Angeles': HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Los+Angeles&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "Geocoder error (attempt 1) for 'Los Angeles': HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Los+Angeles&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "Erreur lors de la création/mise à jour de la table MISSION: can't multiply sequence by non-int of type 'float'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\utcpret\\Documents\\NF26\\nf26_project\\etl_warehouse.py\", line 230, in creer_table_fait_mission\n",
      "    creer_dimension_mission(df_missions)\n",
      "  File \"c:\\Users\\utcpret\\Documents\\NF26\\nf26_project\\etl_warehouse.py\", line 182, in creer_dimension_mission\n",
      "    dim_mission_nouvelles = mission_bilan_carbone(dim_mission_nouvelles)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\utcpret\\Documents\\NF26\\nf26_project\\mission_ges.py\", line 88, in mission_bilan_carbone\n",
      "    GES = GES * 0.001\n",
      "          ~~~~^~~~~~~\n",
      "TypeError: can't multiply sequence by non-int of type 'float'\n",
      "\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LOSANGELES\\BDD_BGES_LOSANGELES_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240906.txt - 1 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_BERLIN\\BDD_BGES_BERLIN_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240907.txt - 1 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_PARIS\\BDD_BGES_PARIS_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240907.txt - 10 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LONDON\\BDD_BGES_LONDON_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240907.txt - 5 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_LOSANGELES\\BDD_BGES_LOSANGELES_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240907.txt - 5 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_NEWYORK\\BDD_BGES_NEWYORK_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240907.txt - 6 lignes\n",
      "Fichier chargé et filtré: BDD_BGES\\BDD_BGES\\BDD_BGES_SHANGHAI\\BDD_BGES_SHANGHAI_INFORMATIQUE\\MATERIEL_INFORMATIQUE_20240907.txt - 2 lignes\n",
      "Table dimension DATE mise à jour avec 1 nouvelles dates\n",
      "Erreur lors de la création/mise à jour de la table MATERIEL: 'DataFrame' object has no attribute 'to_csv'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\utcpret\\Documents\\NF26\\nf26_project\\etl_warehouse.py\", line 291, in creer_table_fait_materiel\n",
      "    creer_dimension_materiel(df_materiel)\n",
      "  File \"c:\\Users\\utcpret\\Documents\\NF26\\nf26_project\\etl_warehouse.py\", line 213, in creer_dimension_materiel\n",
      "    dim_materiel_nouvelles.to_csv(dimension_materiel_path, index=False)\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\utcpret\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\dataframe.py\", line 3129, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'DataFrame' object has no attribute 'to_csv'\n",
      "\n",
      "ETL terminé pour la date: 20240907\n"
     ]
    }
   ],
   "source": [
    "date1 = '20240429'\n",
    "executer_etl_pour_date(date1)\n",
    "date2 = '20240907'\n",
    "executer_etl_pour_date(date2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
